{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakdown_gc():\n",
    "    \"\"\"\n",
    "    This function takes all of the webscraping, cleaning, and manipulating I have done in other notebooks, and combines the entire process into one huge function that asks for significantly more user input. \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ##### Setup #####\n",
    "    \n",
    "\n",
    "\n",
    "    # import necessary packages\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import os\n",
    "    import time\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    import glob\n",
    "    from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    import re\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    # set working directory for accessing and saving files\n",
    "    os.chdir('d:\\\\Faith and Religion Stuff\\\\Come, Follow Me\\\\come-follow-me-breakdown-builder')\n",
    "\n",
    "    # ask for user input for link\n",
    "    conf_link = input('Please paste the link to the landing page of the conference you would like to breakdown.')\n",
    "\n",
    "    # ask for user input for year and month of conference\n",
    "    month = input('Please enter whether the conference was held in April or October: ').lower().replace('il','',1).replace('ober','',1)\n",
    "    year = input('Please enter the year of the conference: ').strip().replace('20','',1)\n",
    "    month_year = month + year\n",
    "\n",
    "    # ask for user input for the start date and end dates\n",
    "    ini_start = str(input(\"What day would you like to start reading?\"))\n",
    "    ini_end = str(input(\"What day would you like to stop reading?\"))\n",
    "\n",
    "    # Convert user input into datetime objects\n",
    "    ini_start_date = pd.to_datetime(ini_start,format='%m/%d/%Y')\n",
    "    ini_end_date = pd.to_datetime(ini_end,format='%m/%d/%Y')\n",
    "    \n",
    "    # Get the total number of days for the breakdown plan\n",
    "    ini_total_days = ini_end_date - (ini_start_date - timedelta(days=1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Getting information such as author, title, and description of each link on the conference landing page. #####\n",
    "\n",
    "\n",
    "\n",
    "    # get response\n",
    "    response = requests.get(conf_link)\n",
    "\n",
    "    # Define the path to the chromedriver executable\n",
    "    chrome_driver_dir = r'D:\\\\Faith and Religion Stuff\\\\Come, Follow Me\\\\chromedriver-win64'\n",
    "    chrome_driver_path = os.path.join(chrome_driver_dir, 'chromedriver.exe')\n",
    "\n",
    "    # Set up the headless browser options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "\n",
    "    # Set up the Chrome service\n",
    "    service = Service(chrome_driver_path)    \n",
    "        \n",
    "    # Initialize the Chrome WebDriver\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    # Establish a try loop that tries to navigate to the provided link and find and store bits of info that we need \n",
    "    try:\n",
    "        # Navigate to the page with your elements - in this case the April 2024 General Conference\n",
    "        driver.get(conf_link)\n",
    "\n",
    "        # Find all elements with the specified class name\n",
    "        # Gotta use dots, not spaces, here because CSS considers each of those spaces to be defining a dif class object\n",
    "        elements = driver.find_elements(By.CSS_SELECTOR, 'a.sc-omeqik-0.ewktus.list-tile.listTile-WHLxI')  \n",
    "\n",
    "        # Initialize a list to store authors, titles, descriptions, and links\n",
    "        primary_meta_list = []\n",
    "        title_list = []\n",
    "        description_list = []\n",
    "        href_list = []\n",
    "\n",
    "        # Iterate over each element\n",
    "        # This for loop will, for all the videos/links to talks on the 2024 General Conference page, run through each of the following operations before moving onto the next\n",
    "        for element in elements:\n",
    "            # Try to get the author\n",
    "            try:\n",
    "                # Finds and stores the primary meta element (which is the author of the talk or report)\n",
    "                primary_meta_element = element.find_element(By.CSS_SELECTOR,'p.primaryMeta')\n",
    "                # Saves the stored author information as text\n",
    "                primary_meta = primary_meta_element.text\n",
    "            # If there is no author, save the author as None or Null\n",
    "            except:\n",
    "                primary_meta = None\n",
    "            # Adds the author (or the None) to the list of authors in the appropriate row\n",
    "            primary_meta_list.append(primary_meta)\n",
    "\n",
    "            # Try to get the title - every link/video should have a title\n",
    "            try:\n",
    "                # Finds and stores the title element (the title of the video, talk, or report)\n",
    "                title_element = element.find_element(By.CSS_SELECTOR,'p.title')\n",
    "                # Saves the stored title as text\n",
    "                title = title_element.text\n",
    "            # If there is no title, save the title as None - THIS SHOULD NEVER BE THE CASE\n",
    "            except:\n",
    "                title = None\n",
    "            # add the title (or the None) to the list of titles in the appropriate row\n",
    "            title_list.append(title)\n",
    "\n",
    "            # Try to get the description - the summary blurb about the video, talk, or report\n",
    "            try:\n",
    "                # Finds and stores the description element (the title of the video, talk, or report)\n",
    "                description_element = element.find_element(By.CSS_SELECTOR,'p.description')\n",
    "                # Saves the stored description as text\n",
    "                description = description_element.text\n",
    "            # If there is no description, save it as None - THIS SHOULD NEVER BE THE CASE\n",
    "            except:\n",
    "                description = None\n",
    "            # add the title (or the None) to the list of descriptions in the appropriate row\n",
    "            description_list.append(description)\n",
    "\n",
    "            # Finds and stores the link (or href) to the video, talk, or report\n",
    "            # This is ultimately going to be the information we use later to get the lengths (in paragraphs) of the talks and the lengths (in lines) of each of those paragraphs\n",
    "            href = element.get_attribute('href')\n",
    "            # Adds the stored href to the list of hrefs in the appropriate row\n",
    "            href_list.append(href)\n",
    "\n",
    "        # Creates a dataframe to store all the found and stored lists together\n",
    "        ini_conf_df = pd.DataFrame({\n",
    "            'Author': primary_meta_list,\n",
    "            'Title': title_list,\n",
    "            'Description': description_list,\n",
    "            'Link': href_list\n",
    "        })\n",
    "\n",
    "    # If anything doesn't work for some reason, tell why\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # After running everything, close the driver we opened to collect the data\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "    ##### Removing things we don't need. #####\n",
    "    \n",
    "\n",
    "\n",
    "    # initialize empty list of rows that need to be dropped\n",
    "    rows_to_drop = []\n",
    "\n",
    "    # Adds the indexes (or row numbers) of rows to the list of rows to be dropped if there is either no Author or Description\n",
    "        ## This exclusionary list is easy to edit\n",
    "    for index,row in ini_conf_df.iterrows():\n",
    "        if row['Author'] == None:\n",
    "            rows_to_drop.append(index)\n",
    "        elif row['Description'] == None:\n",
    "            rows_to_drop.append(index)\n",
    "        elif 'Sustaining' in row['Title']:\n",
    "            rows_to_drop.append(index)\n",
    "        elif 'Audit' in row['Title']:\n",
    "            rows_to_drop.append(index)\n",
    "    \n",
    "    # Drops the rows in the list of rows to drop from the dataframe and resets the index\n",
    "        ## This eliminates from the dataframe the session videos and the sustaining of the officers of the Church\n",
    "    conf_df_1 = ini_conf_df.drop(rows_to_drop).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Getting additional information about each talk. ##### \n",
    "\n",
    "\n",
    "\n",
    "    # Define function for getting the total number of lines all talks \n",
    "        ## This function uses the urls stored in the dataframe\n",
    "    def get_total_lines(url):\n",
    "        \"\"\"\n",
    "        This function was designed specifically to run using an already active webdriver to gather paragraph and line length information about a general conference talk.\n",
    "        First, it uses a webdriver to navigate to a url and then finds the paragraphs within a body block, and gets the size of the rectangles within which each of those paragraphs are assigned to appear. \n",
    "        Then, calculates the height of each line, saves that number as an integer, and calculates how many of those lines would fit into the assigned rectangle. \n",
    "        Then, it adds the paragraph number and the number of lines in that paragraph to the previously created dataframe.\n",
    "        Finally, it calculates and returns the total number of lines in the talk by getting the sum of all paragraph lengths in lines. \n",
    "        \"\"\"\n",
    "        # initialize empty dataframe \"data_list\", with columns \"paragraph\" and \"lines\" being initially populated with NA values\n",
    "        data_list = pd.DataFrame()\n",
    "        data_list['paragraph'] = pd.NA\n",
    "        data_list['lines'] = pd.NA\n",
    "\n",
    "        # Find all elements containing the text\n",
    "        paragraphs = driver.find_elements(By.CSS_SELECTOR, '.body-block p')\n",
    "\n",
    "        # Iterate over each paragraph element\n",
    "        for index, paragraph in enumerate(paragraphs, start=1):\n",
    "            # Log paragraph number, since the paragraphs are not numbered. \n",
    "            paragraph_number = index\n",
    "\n",
    "            # Get the bounding rectangle of the element\n",
    "            rect = paragraph.rect\n",
    "\n",
    "            # Calculate line height\n",
    "            line_height_str = driver.execute_script(\"return window.getComputedStyle(arguments[0]).getPropertyValue('line-height');\", paragraph)\n",
    "            line_height_numeric = int(re.search(r'\\d+', line_height_str).group())  # Extract numeric value from string\n",
    "\n",
    "            # Calculate number of lines\n",
    "            num_lines = rect['height'] // line_height_numeric\n",
    "\n",
    "            # Append data dictionary to list\n",
    "            data_list.at[index,'paragraph'] = paragraph_number\n",
    "            data_list.at[index, 'lines'] = num_lines\n",
    "\n",
    "            total_lines = sum(data_list['lines'])\n",
    "        \n",
    "        return total_lines\n",
    "    \n",
    "    # Initialize the Chrome WebDriver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "    # Copy conf_df_1 to create a reversion point if necessary (it won't be)\n",
    "    conf_df_2 = conf_df_1.copy()\n",
    "    \n",
    "    # Initialize columns in the dataframe with NA values to later be filled\n",
    "    conf_df_2['time'] = pd.NA\n",
    "    conf_df_2['paragraphs'] = pd.NA\n",
    "    conf_df_2['lines'] = pd.NA\n",
    "    conf_df_2['role'] = pd.NA\n",
    "\n",
    "    # iterate the following over each row in the apr_2024_df dataframe\n",
    "    for index, row in conf_df_2.iterrows():\n",
    "        # for each row, when the function calls for title, url, and author it is looking for the Title, Link, and Author columns in that row, respectively\n",
    "        title = row['Title']\n",
    "        url = row['Link']\n",
    "        author = row['Author']\n",
    "\n",
    "        # run the driver, navigating to the linked page in the row currently being worked on\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to load completely\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        # Simulate clicking the play button using the class attribute\n",
    "        try:\n",
    "            play_button = driver.find_element(By.CSS_SELECTOR, \"button.sc-1g7hsbc-0.lcWZjw.sc-bvqtyr-4.eYHLNi\")\n",
    "            play_button.click()\n",
    "            print(f\"Clicked the play button for {title} to start the media.\")\n",
    "        except:\n",
    "            print(f\"Play button for {title} not found.\")\n",
    "\n",
    "        # Wait for the video element to be present in the DOM\n",
    "        try:\n",
    "            video_element = WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, 'video'))\n",
    "            )\n",
    "            print(\"Video element found in the DOM.\")\n",
    "        except:\n",
    "            print(\"No video element found.\")\n",
    "\n",
    "        # Wait for a short period to allow the video to start loading\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Attempt to retrieve the video duration using JavaScript\n",
    "        try:\n",
    "            video_duration = driver.execute_script(\"\"\"\n",
    "                let video = document.querySelector('video');\n",
    "                if (video) {\n",
    "                    console.log('Video element is present, checking duration...');\n",
    "                    return video.duration;\n",
    "                } else {\n",
    "                    let audio = document.querySelector('audio');\n",
    "                    if (audio) {\n",
    "                        console.log('Audio element is present, checking duration...');\n",
    "                        return audio.duration;\n",
    "                    }\n",
    "                }\n",
    "                return null;  // No media element found\n",
    "            \"\"\")\n",
    "            \n",
    "            # if video_duration exists\n",
    "            if video_duration:\n",
    "                # print a message saying how long the talk is in seconds\n",
    "                print(f\"{title} duration: {video_duration:.2f} seconds\")\n",
    "                # save the duration into the dataframe in the same row\n",
    "                conf_df_2.at[index, 'time'] = video_duration\n",
    "            # otherwise, print a message saying no video or audio element was found for the talk\n",
    "            else:\n",
    "                print(f\"No video or audio element found for {title}.\")\n",
    "        \n",
    "        # If there is an error, say there was an error and what it was, and try to get the next piece of information       \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving video duration for {title}: {e}\")\n",
    "\n",
    "        # attempt to find the body block\n",
    "        try:\n",
    "            paragraphs = driver.find_elements(By.CSS_SELECTOR, '.body-block p')\n",
    "            # if body block is found, find the number of paragraphs, and save that number to the dataframe in the same row\n",
    "            conf_df_2.at[index, 'paragraphs'] = len(paragraphs)\n",
    "            # print a message giving the length of the talk in paragraphs\n",
    "            print(f\"Paragraph length of {title}: {len(paragraphs)} paragraphs.\")\n",
    "\n",
    "            # Use the get_total_lines function to get the total number of lines in the talk\n",
    "            num_lines = get_total_lines(url)\n",
    "            \n",
    "            # save the number of lines to the dataframe in the same row\n",
    "            conf_df_2.at[index, 'lines'] = num_lines\n",
    "            \n",
    "            # print a message telling the number of lines in the talk\n",
    "            print(f\"Line length of {title}: {num_lines} lines.\")\n",
    "\n",
    "        # if there is an error or a problem, print a message saying what the problem was, and try to get the next piece of information \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating lines and paragraphs for {title}: {e}\")\n",
    "        \n",
    "        # try to find the author role\n",
    "        try:\n",
    "            role = driver.find_element(By.CLASS_NAME, 'author-role')\n",
    "            # if there is one, save it to the dataframe in the same row\n",
    "            conf_df_2.at[index, 'role'] = role.text\n",
    "            # print a message displaying the role of the author\n",
    "            print(f\"Role of {author}: {role.text}\")\n",
    "        # if there is an error or a problem, print a message saying what the problem was and then move onto the next row\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving role for {author}: {e}\")\n",
    "\n",
    "    # Close the browser after all rows have been iterated through\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "    # converts all numeric columns to integers for easier use later\n",
    "    conf_df_2['time'] = conf_df_2['time'].astype(int).round(0)\n",
    "    conf_df_2['paragraphs'] = conf_df_2['paragraphs'].astype(int)\n",
    "    conf_df_2['lines'] = conf_df_2['lines'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    ##### Establishing a primary key column and getting read and day weights\n",
    "    \n",
    "\n",
    "\n",
    "    # copy conf_df_2 to establish a reversion point\n",
    "    conf_df_3 = conf_df_2.copy()\n",
    "\n",
    "    # copies the role column onto a newly created short_role column\n",
    "    conf_df_3['short_role'] = conf_df_3['role']\n",
    "\n",
    "    # initializes a replacement dictionary to shorten information in newly created 'short_role' column\n",
    "    rep_dict = {}\n",
    "\n",
    "    # Adds specific shortenings of each role to the replacement dictionary\n",
    "        ## this list is also easily editable if any other office becomes prominently represented in future conferences\n",
    "        ## this list also puts members of the Presidency of the Seventy and of any other member of any other Quorum of the Seventy on equal ground       \n",
    "    for index, row in conf_df_3.iterrows():\n",
    "        if 'President of The Church'in row['role']:\n",
    "            rep_dict[row['short_role']] = 'President of the Church'\n",
    "        elif 'First Presidency' in row['role']:\n",
    "            rep_dict[row['short_role']] = 'First Presidency'\n",
    "        elif 'Quorum of the Twelve' in row['role']:\n",
    "            rep_dict[row['short_role']] = 'Quorum of the Twelve'\n",
    "        elif 'the Seventy' in row['role']:\n",
    "            rep_dict[row['short_role']] = 'Seventy'\n",
    "        elif 'Relief Society' in row['role']:\n",
    "            rep_dict[row['short_role']] = 'Relief Society Presidency'\n",
    "        elif 'Presiding' in row['role']:\n",
    "            rep_dict[row['short_role']] = 'Presiding Bishopric'\n",
    "        elif 'Sunday School' in row['role']:\n",
    "            rep_dict[row['short_role']] = 'Sunday School Presidency'\n",
    "        elif 'Young Men' in row['role']:\n",
    "            rep_dict[row['short_role']] = 'Young Men Presidency'\n",
    "        elif 'Young Women' in row['role']:\n",
    "            rep_dict[row['short_role']] = 'Young Women Presidency'\n",
    "        elif 'Primary' in row['role']:\n",
    "            rep_dict[row['short_role']] = 'Primary Presidency'\n",
    "        else:\n",
    "            rep_dict[row['short_role']] = 'other speakers'\n",
    "    \n",
    "    # uses replacement dictionary to replace (shorten) all the entries in the short_role column\n",
    "    for words, replacement in rep_dict.items():\n",
    "        conf_df_3['short_role'] = conf_df_3['short_role'].replace(words, replacement).str.strip()\n",
    "    \n",
    "    # defines function to get the initials of the speaker for use in creation of primary key column\n",
    "    def get_initials(full_name):\n",
    "        parts = full_name.split()\n",
    "        initials = [part[0].lower() for part in parts]\n",
    "        return ''.join(initials)\n",
    "\n",
    "    # Create a new column with initials\n",
    "    conf_df_3['initials'] = conf_df_3['Author'].apply(get_initials)\n",
    "\n",
    "    # create a primary key column that combines the initials of the speaker and the month and year of the conference\n",
    "    conf_df_3['pk'] = (conf_df_3['initials'] + \"_\" + month_year)\n",
    "\n",
    "    # initialize an empty list of read weights\n",
    "    read_weights = []\n",
    "\n",
    "    # for every unique role code in the role_code column of the apr_2024_info dataframe...\n",
    "    for short_role in conf_df_3.short_role.unique():\n",
    "        # ... ask the user what the read weight should be and...\n",
    "        read_weight = int(input(f\"How many times would you like to read talks given by the {short_role}?\"))\n",
    "        # ... save both the role code and the read weight to the read_weights list\n",
    "        read_weights.append({'short_role':short_role, 'read_weight':read_weight})\n",
    "\n",
    "    # convert the read_weights list to a dataframe, save with the same name to replace the old item\n",
    "    read_weights = pd.DataFrame(read_weights)\n",
    "\n",
    "    # left-merge the read_weights dataframe to the apr_2024_info dataframe useing the role_code columns as a guide for merging\n",
    "    # left-merge keeps everything in the dataframe being merged to, and only merges data from the second dataframe that has a corresponding value in the original dataframe\n",
    "    conf_df_3 = conf_df_3.merge(read_weights, on='short_role',how='left')\n",
    "\n",
    "    # initialize an empty list of day weights\n",
    "    day_weights = []\n",
    "\n",
    "    # for every unique role code in the role_code column of the apr_2024_info dataframe...\n",
    "    for short_role in conf_df_3.short_role.unique():\n",
    "        # ... ask the user what the day weight should be and...\n",
    "        day_weight = int(input(f\"How many more or fewer days would you like to spend on talks given by the {short_role}?\\n\"\n",
    "                               f\"\\nIf you want to spend more days reading talks from the {short_role}, enter a number above 0.\\n\"\n",
    "                               f\"\\nOr if you want to spend fewer days reading talks from the {short_role}, enter a number below 0 by using a minus sign or dash.\\n\"\n",
    "                               f\"\\nIf you would rather spend a relatively the same amount of time on each talk from this organization as others, enter 0.\"))\n",
    "        # ... save both the role code and the day weight to the read_weights list\n",
    "        day_weights.append({'short_role':short_role, 'day_weight':day_weight})\n",
    "\n",
    "    # convert the read_weights list to a dataframe, save with the same name to replace the old item\n",
    "    day_weights = pd.DataFrame(day_weights)\n",
    "\n",
    "    # left-merge the read_weights dataframe to the apr_2024_info dataframe useing the role_code columns as a guide for merging\n",
    "    # left-merge keeps everything in the dataframe being merged to, and only merges data from the second dataframe that has a corresponding value in the original dataframe\n",
    "    conf_df_3 = conf_df_3.merge(day_weights, on='short_role',how='left')\n",
    "    \n",
    "\n",
    "\n",
    "    ##### Getting information about each talk\n",
    "\n",
    "\n",
    "\n",
    "    # save a copy of conf_df_3 as a reversion point\n",
    "    conf_df_4 = conf_df_3.copy()\n",
    "\n",
    "    # define a function that takes a link and gets the text and counts the lines of text of each talk given in the linked conference\n",
    "    def get_talks(talk_link):\n",
    "        \"\"\"\n",
    "        This function finds the description and text of a talk found at the talk link, numbers each paragraph of that talk and gets the text and length in lines of each paragraph. It returns as a dataframe all this information about each talk. \n",
    "        \"\"\"\n",
    "\n",
    "        # Define the path to the chromedriver executable\n",
    "        chrome_driver_dir = r'D:\\\\Faith and Religion Stuff\\\\Come, Follow Me\\\\chromedriver-win64'\n",
    "        chrome_driver_path = os.path.join(chrome_driver_dir, 'chromedriver.exe')\n",
    "\n",
    "        # Set up the headless browser options\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "\n",
    "        # Set up the Chrome service\n",
    "        service = Service(chrome_driver_path)    \n",
    "        \n",
    "        # Initialize the Chrome WebDriver\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        # Run the driver\n",
    "        driver.get(talk_link)\n",
    "\n",
    "        # Initialize a list to store data dictionaries\n",
    "        data_list = []\n",
    "\n",
    "        # Try to get the description - the summary blurb about the video, talk, or report\n",
    "        try:\n",
    "            # Finds and stores the description (kicker) element (the title of the video, talk, or report)\n",
    "            description_element = driver.find_element(By.CSS_SELECTOR,'p.kicker')\n",
    "            # Saves the stored description as text\n",
    "            description = description_element.text\n",
    "\n",
    "            # set paragraph number for description as 0\n",
    "            paragraph_number = 0\n",
    "            \n",
    "            # Get the bounding rectangle of the element\n",
    "            rect = description_element.rect\n",
    "\n",
    "            # Calculate line height\n",
    "            line_height_str = driver.execute_script(\"return window.getComputedStyle(arguments[0]).getPropertyValue('line-height');\", description_element)\n",
    "            line_height_numeric = int(re.search(r'\\d+', line_height_str).group())  # Extract numeric value from string\n",
    "\n",
    "            # Calculate number of lines\n",
    "            num_lines = rect['height'] // line_height_numeric\n",
    "\n",
    "            # add text of and information about description to the data_list dictionary\n",
    "            data_list.append({\n",
    "                'paragraph_number': paragraph_number,\n",
    "                'num_lines': num_lines,\n",
    "                'text': description\n",
    "            })\n",
    "\n",
    "        # If there is no description, save it as None - THIS SHOULD NEVER BE THE CASE\n",
    "        except:\n",
    "            description = None\n",
    "\n",
    "        # Find all elements containing the text\n",
    "        paragraphs = driver.find_elements(By.CSS_SELECTOR, '.body-block p')\n",
    "\n",
    "        # Iterate over each paragraph element\n",
    "        for index, paragraph in enumerate(paragraphs, start=1):\n",
    "            # Get the text of the element\n",
    "            text = paragraph.text\n",
    "\n",
    "            # Log paragraph number, since the paragraphs are not numbered. \n",
    "            paragraph_number = index\n",
    "\n",
    "            # Get the bounding rectangle of the element\n",
    "            rect = paragraph.rect\n",
    "\n",
    "            # Calculate line height\n",
    "            line_height_str = driver.execute_script(\"return window.getComputedStyle(arguments[0]).getPropertyValue('line-height');\", paragraph)\n",
    "            line_height_numeric = int(re.search(r'\\d+', line_height_str).group())  # Extract numeric value from string\n",
    "\n",
    "            # Calculate number of lines\n",
    "            num_lines = rect['height'] // line_height_numeric\n",
    "\n",
    "            # Append data dictionary to list\n",
    "            data_list.append({\n",
    "                'paragraph_number': paragraph_number,\n",
    "                'num_lines': num_lines,\n",
    "                'text': text\n",
    "            })\n",
    "\n",
    "        # Convert list of dictionaries to DataFrame\n",
    "        df = pd.DataFrame(data_list)\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "        return df\n",
    "    \n",
    "    # Define directory path for saving CSV files\n",
    "    dir_path = input(f'Please paste here the location of the folder in which you would like to store information from this general conference.\\n'\n",
    "                     f'\\nExample:    D:\\\\Faith and Religion Stuff\\\\Come, Follow Me Breakdowns\\\\April 2024 GC Talks\\n'\n",
    "                     f'\\nThis will require you to have already created a folder in which you want the information for the talks to be saved.')\n",
    "\n",
    "    # establish a loop that iterates through every row of the conference dataframe\n",
    "    for index, row in conf_df_4.iterrows():\n",
    "        # save information from the 'Title' row as title\n",
    "        title = row['Title']\n",
    "        # save information from the 'Link' row as link\n",
    "        link = row['Link']\n",
    "        # save information from the 'pk' Primary Key row as foreign_key\n",
    "        foreign_key = row['pk']\n",
    "\n",
    "        # using the saved link, create a dataframe that contains the paragraph line counts information for the talk in the current row \n",
    "        talk_lines_df = get_talks(link)\n",
    "\n",
    "        # if the created dataframe is not populated with None data and is not empty:\n",
    "        if talk_lines_df is not None and not talk_lines_df.empty:\n",
    "            # creates a new column in the dataframe that uses the primary key of the talk as the foreign key\n",
    "            talk_lines_df['foreign_key'] = foreign_key\n",
    "            # Uses a lambda function to create a primary key for each paragraph consisting of the foreign key + the paragraph number\n",
    "            talk_lines_df['pk'] = talk_lines_df.apply(lambda x:f\"{foreign_key}_{x.get('paragraph_number')}\", axis=1)\n",
    "        \n",
    "            # very rudimentarily define which columns to keep, and add column names to that list in the desired order\n",
    "            columns_to_keep = ['foreign_key','pk']\n",
    "            columns_to_keep.append('paragraph_number')\n",
    "            columns_to_keep.append('text')\n",
    "            columns_to_keep.append('num_lines')\n",
    "\n",
    "            # save dataframe with columns in the order specified in the columns_to_keep list\n",
    "            talk_lines_df = talk_lines_df[columns_to_keep]\n",
    "            \n",
    "            # save file name\n",
    "            csv_filename = f'{foreign_key}_lines.csv'\n",
    "            # combine the file name and the user inputted folder location to create a complete save path\n",
    "            full_path = os.path.join(dir_path,csv_filename)\n",
    "\n",
    "            # Debugging line to state where files can be found\n",
    "            print(f'Saving to: {full_path}')\n",
    "\n",
    "            # export dataframe as a csv file to the location specified\n",
    "            talk_lines_df.to_csv(full_path, index = False)\n",
    "            # print message saying that csv file has been created\n",
    "            print(f'CSV file for \"{title}\" saved successfully as {csv_filename}.')\n",
    "        \n",
    "        # otherwise, if the dataframe is filled with None values or is empty\n",
    "        else:\n",
    "            # print a message saying no data was found for the talk\n",
    "            print(f'No data found for \"{title}\", skipping CSV creation.')\n",
    "    \n",
    "    ### stupidly import data I just exported because I don't have the bandwidth to come up with another solution and want to go to bed ###\n",
    "\n",
    "    # define the beginning of the file location\n",
    "        ## this is done by accessing the dir_path given by the user, and adding \\\\ to the end of it.\n",
    "    path_start = f'{dir_path}\\\\'\n",
    "\n",
    "    # initialize filenames list\n",
    "    csv_files = []\n",
    "\n",
    "    # get the names of all the csv files in the directory\n",
    "    for file in os.listdir(path_start):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_files.append(file)\n",
    "\n",
    "    # initialize a dictionary to store the dataframes\n",
    "    all_talks_dict = {}\n",
    "\n",
    "    # import the csv files into pandas dataframes, store each dataframe in the dictionary\n",
    "    for file in csv_files:\n",
    "        talk = file[:-4]\n",
    "        all_talks_dict[talk] = pd.read_csv(os.path.join(path_start, file))\n",
    "        print(f'file string: {file}\\n'\n",
    "            f'talk string: {talk}')\n",
    "\n",
    "    # iterate over every dataframe stored in the all_talks_df dictionary\n",
    "    for talk, df in all_talks_dict.items():\n",
    "        # create a new column in each dataframe that is the cumulative sum of the number of lines\n",
    "        df['running_lines'] = df['num_lines'].cumsum()\n",
    "\n",
    "\n",
    "    \n",
    "    ##### Assigning a number of days for each readthrough of each talk #####\n",
    "\n",
    "\n",
    "    \n",
    "    # Since conf_df_4 was not altered in anyway during the previous major step, we don't need to save a copy\n",
    "\n",
    "    # Get some information about the conference as a whole\n",
    "    total_time = sum(conf_df_4['time'])\n",
    "    total_lines = sum(conf_df_4['lines'])\n",
    "    \n",
    "    # create a new conference consumption column that gives a proportional weight to each talk based on it's length and user input\n",
    "    conf_df_4['conf_cons'] = (\n",
    "        ((1/32) +                                                                  # Each talk is 1 of 32 given, this treats each equally\n",
    "        conf_df_4['time']/total_time +                                             # time weight - longer \"heavier\"\n",
    "        conf_df_4['lines']/total_lines +                                           # lines weight - longer \"heavier\"\n",
    "        ((conf_df_4['day_weight'] + 1)/(conf_df_4['day_weight'] + 1).sum()))       # preference weight - user input factors in here\n",
    "        / 4                                                                        # Adding each of those and then dividing by 4 gets the average\n",
    "    )\n",
    "    \n",
    "    # create a column with the total number of days to be spent on each talk\n",
    "        ## multiply the number of days specified in the plan by the conference consumption ratio\n",
    "    conf_df_4['tot_num_days'] = ini_total_days.days * conf_df_4['conf_cons']\n",
    "    conf_df_4['tot_num_days'] = conf_df_4['tot_num_days'].round()\n",
    "\n",
    "    # convert the newly created column into integers rather than floats\n",
    "    conf_df_4['tot_num_days'] = conf_df_4['tot_num_days'].astype(int)\n",
    "\n",
    "    # find and save the highest number in the read_weights column\n",
    "    max_reads = conf_df_4.read_weight.max()\n",
    "\n",
    "    # start a loop that, for every number between 1 and whatever the max_reads number is, inclusive...\n",
    "    for i in range(1,max_reads+1):\n",
    "        # create a new column of NA values titled \"Readthrough # _(whatever number the loop is on)_\"\n",
    "        conf_df_4[f\"Readthrough #{i}\"] = pd.NA\n",
    "        \n",
    "    # convert all NA values to \"0\"\n",
    "    conf_df_4.fillna(0, inplace=True)\n",
    "\n",
    "    def distribute_days(conf_df):\n",
    "        \"\"\"\n",
    "        This function takes a dataframe like the one I have crafted above and distributes the total number of days into the \"Readthrough #_\" columns.\n",
    "        \"\"\"\n",
    "        # establish that the function needs to repeat for every row of the dataframe\n",
    "        for index, row in conf_df.iterrows():\n",
    "            # get total number of days for that talk\n",
    "            total_days = row['tot_num_days']\n",
    "            # initialize number of distributed days as 0\n",
    "            dist_days = 0\n",
    "            # establish that the function needs to proceed with the following operation until dist_days and total_days are equal\n",
    "            while dist_days < total_days:\n",
    "                # for every whole number between 1 and whatever the read_weight (or number of readthroughs) is...\n",
    "                for i in range(1,row['read_weight']+1):\n",
    "                    # if dist_days is still less than total_days...\n",
    "                    if dist_days < total_days:\n",
    "                        # add 1 to whatever value is in the \"Readthrough #(number between 1 and number of readthroughs)\" column and...\n",
    "                        conf_df.at[index, f'Readthrough #{i}'] += 1\n",
    "                        # add 1 to dist_days\n",
    "                        dist_days += 1\n",
    "                        # go back to add 1 to the next column until dist_days is no longer less than total_days\n",
    "                    # if/when dist_days is equal to total_days\n",
    "                    else:\n",
    "                        # break the process of adding one to each column, and move on to the next row to start the process over\n",
    "                        break\n",
    "        # when everything is done, the output of this function is the same dataframe with all of the updated columns\n",
    "        return conf_df\n",
    "\n",
    "    # run the function on my dataframe\n",
    "    conf_df_5 = distribute_days(conf_df_4)\n",
    "\n",
    "\n",
    "\n",
    "    ##### Distributing lines from each talk across each day of each readthrough #####\n",
    "\n",
    "    # Use a series of loops to create a line start, number of lines, line end, paragraph start, and paragraph end column for every day of every readthrough of every talk\n",
    "    for i in range(1, conf_df_5['read_weight'].max()+1):\n",
    "        for x in range(1, conf_df_5[f'Readthrough #{i}'].max()+1):\n",
    "            for index, row in conf_df_5.iterrows():\n",
    "                if row[f'Readthrough #{i}'] != 0:\n",
    "                    conf_df_5.at[index, f'r{i}d{x}_l_start'] = int(0)\n",
    "                    conf_df_5.at[index, f'r{i}d{x}_lines'] = int(0)\n",
    "                    conf_df_5.at[index, f'r{i}d{x}_l_end'] = int(0)\n",
    "                    conf_df_5.at[index, f'r{i}d{x}_p_start'] = int(0)\n",
    "                    conf_df_5.at[index, f'r{i}d{x}_p_end'] = int(0)\n",
    "                else:\n",
    "                    conf_df_5.at[index, f'r{i}d{x}_l_start'] = pd.NA\n",
    "                    conf_df_5.at[index, f'r{i}d{x}_lines'] = pd.NA\n",
    "                    conf_df_5.at[index, f'r{i}d{x}_l_end'] = pd.NA\n",
    "                    conf_df_5.at[index, f'r{i}d{x}_p_start'] = pd.NA\n",
    "                    conf_df_5.at[index, f'r{i}d{x}_p_end'] = pd.NA\n",
    "    \n",
    "    # establishes a regular expression pattern\n",
    "    pattern = r'\\D\\d\\D\\d+'\n",
    "    # identifies columns that are floats and not integers\n",
    "    float_cols = conf_df_5.select_dtypes(include=['float'])\n",
    "    # targets specific float columns using regular expression pattern\n",
    "    change_cols = [col for col in float_cols if re.search(pattern, col)]\n",
    "    # converts targeted columns to integers\n",
    "    conf_df_5[change_cols] = conf_df_5[change_cols].astype(int)\n",
    "\n",
    "    # I struggled for about 5 hours because I forgot this step. Including it was as simple as a copy paste.\n",
    "    # This also came in answer to my prayer for help. I prayed for God to help me know where to look, and this was the next thing I looked at. \n",
    "    # God is good. \n",
    "    \n",
    "    def distribute_lines(conf_df):\n",
    "        \"\"\"\n",
    "        This function takes a dataframe like the one I have crafted above and distributes the total number of days into the \"Readthrough #_\" columns.\n",
    "        \"\"\"\n",
    "        # initialize readthrough count as zero\n",
    "        readthrough = 0\n",
    "        # establish maximum number of readthroughs so the loop I create below knows when to end or stop\n",
    "        max_readthroughs = conf_df['read_weight'].max()\n",
    "        # start a loop of action that will continue until readthroughs is bigger than max_readthroughs, at which point it will stop\n",
    "        while readthrough <= max_readthroughs:\n",
    "            # A - first action: add 1 to readthroughs, establishing which readthrough we are working with\n",
    "            readthrough += 1\n",
    "            # B - Check if the current readthrough exists in the DataFrame\n",
    "            if f'Readthrough #{readthrough}' not in conf_df.columns:\n",
    "                break  # If the column doesn't exist, exit the loop\n",
    "\n",
    "            # C - second action: for every row in the dataframe do the following:\n",
    "            for index, row in conf_df.iterrows():\n",
    "                # D - second action, cont'd: save the info in the row 'lines' as the total number of lines to be distributed\n",
    "                total_lines = row['lines']\n",
    "                # E - second action, cont'd: initialize count of distributed lines as zero\n",
    "                dist_lines = 0\n",
    "                # debugging print line - remove later\n",
    "                print(f\"Row {index} - Readthrough {readthrough} has {row[f'Readthrough #{readthrough}']} days.\")\n",
    "                # F - check whether the number of days assigned to a talk in a particular readthrough is 0\n",
    "                if row[f'Readthrough #{readthrough}'] == 0:\n",
    "                    # debugging print line - remove later\n",
    "                    print(f\"Skipping row {index} - Readthrough {readthrough} because it has 0 days.\")\n",
    "                    # if that talk has zero days alloted for that readthrough, skip to the next talk\n",
    "                    continue\n",
    "                \n",
    "                # G - second action, cont'd: open another while loop that will continue until the number of distributed lines is equal to the number of total lines\n",
    "                while dist_lines < total_lines:\n",
    "                    # H - first action of second loop: open another loop that operates i number of times, where i is the number of days in the readthrough\n",
    "                    for i in range(1, row[f'Readthrough #{readthrough}']+1):\n",
    "                        # debugging print line - remove later\n",
    "                        print(f\"Processing row {index}, readthrough {readthrough}, day {i}.\")\n",
    "                        # I - first action of second loop, cont'd: add 1 to the corresponding i day of the current readthrough\n",
    "                        conf_df.at[index, f'r{readthrough}d{i}_lines'] += 1\n",
    "                        # J - first action of second loop, cont'd: add 1 to the number of distributed lines\n",
    "                        dist_lines += 1\n",
    "                        # debugging print line - remove later\n",
    "                        print(f\"Distributed lines: {dist_lines}/{total_lines}\")\n",
    "                        # K - check if dist_lines is less than total_lines: \n",
    "                        if dist_lines >= total_lines:\n",
    "                            break\n",
    "                            # Unstated action: If it is less, return to point H and repeat this loop.\n",
    "                            # If not, end *this* loop and return to point D for the next row.\n",
    "                    \n",
    "            # L - check if readthroughs is less than or equal to max_readthroughs\n",
    "            if readthrough > max_readthroughs:\n",
    "                # If it is, repeat this loop, starting with point A\n",
    "                break\n",
    "                # Unstated action: If not, end this loop and go to point M.\n",
    "        \n",
    "        # M - return the newly modified dataframe\n",
    "        return conf_df\n",
    "    \n",
    "    conf_df_5 = distribute_lines(conf_df_5)\n",
    "    \n",
    "    ##### Assigning lines and paragraphs for each day of each readthrough. Final Major Step. #####\n",
    "\n",
    "\n",
    "    # save a copy of conf_df_5 as a reversion point\n",
    "    conf_df_6 = conf_df_5.copy()\n",
    "    \n",
    "\n",
    "    def get_paragraphs(conf_df,talks_dictionary):\n",
    "        \"\"\"\n",
    "        This function assigns starting and ending lines and paragraphs for each day of each readthrough of each talk contained in the conference dataframe.\n",
    "        \"\"\"    \n",
    "        # initialize readthrough count as 0\n",
    "        rt = 0\n",
    "        # establish maximum number of readthroughs so the loop I create below knows when to end or stop\n",
    "        max_rts = conf_df['read_weight'].max()\n",
    "        # create a list of the column names in the given dataframe\n",
    "        cols = list(conf_df.columns)\n",
    "        # start a loop of action that will continue until readthroughs is bigger than max_readthroughs, at which point it will stop\n",
    "        while rt <= max_rts:\n",
    "            # A - first action: add 1 to readthroughs, establishing which readthrough we are working with\n",
    "            rt += 1\n",
    "            # debugging line - remove later\n",
    "            print(f'Starting readthrough {rt}.')\n",
    "            # B - Check if the current readthrough exists in the DataFrame\n",
    "            if f'Readthrough #{rt}' not in conf_df.columns:\n",
    "                # debugging line - remove later\n",
    "                print(f'Readthrough {rt} not found. Function complete.')\n",
    "                # if a column for the current readthrough number doesn't exist, exit the loop\n",
    "                break\n",
    "\n",
    "            # C-1 - Second action: start another loop that does the follow for the highest number of days in whatever readthrough number the function is on\n",
    "            for day in range(1, conf_df[f'Readthrough #{rt}'].max()+1):\n",
    "                \n",
    "                # debugging line - remove later\n",
    "                print(f'Working through readthrough {rt} day {day}.')\n",
    "\n",
    "                # C-2 - set patterns for pulling out info for the start, lines, end, and start of next day\n",
    "                rt_start_l_pattern = fr'r{rt}d{day}_l_start'\n",
    "                rt_lines_pattern = fr'r{rt}d{day}_lines'\n",
    "                rt_end_l_pattern = fr'r{rt}d{day}_l_end'\n",
    "                rt_next_start_l_pattern = fr'r{rt}d{day+1}_l_start'\n",
    "                rt_start_p_pattern = fr'r{rt}d{day}_p_start'\n",
    "                rt_end_p_pattern = fr'r{rt}d{day}_p_end'\n",
    "                rt_next_start_p_pattern = fr'r{rt}d{day+1}_p_start'\n",
    "\n",
    "                # debugging line - remove later\n",
    "                print(f'Readthrough {rt} day {day} patterns saved.')\n",
    "\n",
    "                # C-3 - initialize variables as empty lists\n",
    "                start_l_col = []\n",
    "                lines_col = []\n",
    "                end_l_col = []\n",
    "                next_start_l_col = []\n",
    "                start_p_col = []\n",
    "                end_p_col = []\n",
    "                next_start_p_col = []\n",
    "\n",
    "                # debugging line - remove later\n",
    "                print(f'Variables initialized.')\n",
    "\n",
    "                # C-4 - search for and save columns defined in patterns\n",
    "                for col in cols:\n",
    "                    if re.search(rt_start_l_pattern, col):\n",
    "                        start_l_col.append(col)\n",
    "                    elif re.search(rt_lines_pattern, col):\n",
    "                        lines_col.append(col)\n",
    "                    elif re.search(rt_end_l_pattern, col):\n",
    "                        end_l_col.append(col)\n",
    "                    elif re.search(rt_next_start_l_pattern, col):\n",
    "                        next_start_l_col.append(col)\n",
    "                    elif re.search(rt_start_p_pattern, col):\n",
    "                        start_p_col.append(col)\n",
    "                    elif re.search(rt_end_p_pattern, col):\n",
    "                        end_p_col.append(col)\n",
    "                    elif re.search(rt_next_start_p_pattern, col):\n",
    "                        next_start_p_col.append(col)\n",
    "\n",
    "                # debugging line - remove later\n",
    "                print(f'Columns found and saved.'\n",
    "                    f'Starting column name: {start_l_col}'\n",
    "                    f'Number of lines column: {lines_col}'\n",
    "                    f'End column name: {end_l_col}'\n",
    "                    f'Next start column name: {next_start_l_col}'\n",
    "                    f'Starting column name: {start_p_col}'\n",
    "                    f'Number of lines column: {lines_col}'\n",
    "                    f'End column name: {end_p_col}'\n",
    "                    f'Next start column name: {next_start_p_col}')\n",
    "                \n",
    "                # C-5-a -Start another loop\n",
    "                for index, row in conf_df.iterrows():\n",
    "                    # C-5-b - check if the number of days assigned for the current readthrough of the current talk is 0\n",
    "                    if row[f'Readthrough #{rt}'] == 0:\n",
    "                        # debugging print line - remove later\n",
    "                        print(f\"Skipping row {index} - Readthrough {rt} because it has 0 days.\")\n",
    "                        # if that talk has zero days alloted for that readthrough, skip to the next talk\n",
    "                        continue\n",
    "                    \n",
    "                    # unstated action - if the number of days assigned for the current readthrough is greater than zero, proceed to C-5-c\n",
    "\n",
    "                    # C-5-c - establish the connection between conf_df and talks_dict using the primary key column of the conf_df\n",
    "                    talk = talks_dictionary[f\"{conf_df.loc[index,'pk']}_lines\"]\n",
    "\n",
    "                    # debugging line - remove later\n",
    "                    print(f'Connecting conf_df to talks_dict using primary key {conf_df.loc[index,\"pk\"]}.')\n",
    "\n",
    "                    # C-5-d - check if the day number is 1\n",
    "                    if day == 1:\n",
    "                        # C-5-d-1 - if so, initialize the start column as 1 for every row\n",
    "                        conf_df[start_l_col] = 1\n",
    "                        conf_df[start_p_col] = 1\n",
    "                        # debugging line - remove later\n",
    "                        print(f'Readthrough {rt} day {day} set at 1.')\n",
    "                        # C-5-d-1 - then save the end point as the start point (1) plus the number of lines to be read \n",
    "                        # the minus 1 at the end ensures that we end at the assigned reading line, not the line after\n",
    "                        for start, read, end in zip(start_l_col, lines_col, end_l_col):\n",
    "                            conf_df.loc[index, end] = conf_df.loc[index, start] + conf_df.loc[index, read] - 1\n",
    "                        # C-5-d-2 - then save today's ending point as the starting point for the next day, to be accessed later\n",
    "                        for end, next_start in zip(end_l_col, next_start_l_col):\n",
    "                            conf_df.loc[index, next_start] = conf_df.loc[index, end]\n",
    "                        # C-5-d-3 - save the end line just calculated as object 'end_line'\n",
    "                        end_line = conf_df.loc[index, f'r{rt}d{day}_l_end']\n",
    "\n",
    "                        # debugging line - remove later\n",
    "                        print(f'Readthrough {rt} day {day} end line set. End line: {end_line}')\n",
    "\n",
    "                        # C-5-d-4 - find in the talk the paragraph with a running total that is greater than or equal to the end line \n",
    "                        end_paragraph_1 = talk.loc[talk['running_lines'] >= end_line].index[0]\n",
    "                        # C-5-d-5 - find in the talk the paragraph before the one above\n",
    "                        end_paragraph_2 = talk.loc[talk['running_lines'] >= end_line].index[-1]\n",
    "                        # C-5-d-6 - determine which paragraph would yield a number of lines being read closer to the target end line and save as end_paragraph\n",
    "                        if abs(talk.loc[end_paragraph_1,'running_lines'] - end_line) < abs(talk.loc[end_paragraph_2,'running_lines']- end_line):\n",
    "                            end_paragraph = end_paragraph_1\n",
    "                        else:\n",
    "                            end_paragraph = end_paragraph_2\n",
    "                        # C-5-d-7 - save the selected end_paragraph as the paragraph ending point of the current day of the current readthrough\n",
    "                        for end_p in end_p_col:\n",
    "                            conf_df.loc[index, end_p] = end_paragraph\n",
    "                        \n",
    "                        for end_p, next_start_p in zip(end_p_col, next_start_p_col):\n",
    "                            conf_df.loc[index, next_start_p] = conf_df.loc[index, end_p] + 1\n",
    "\n",
    "                        # debugging line - remove later\n",
    "                        print(f'Readthrough {rt} day {day} end paragraph set: End paragraph: {end_paragraph}')\n",
    "                        \n",
    "\n",
    "                        \n",
    "                    \n",
    "                    # C-5-e - if the day number IS NOT 1...\n",
    "                    else:\n",
    "                        # C-5-e-1 - access whatever the start column has already been saved as, add the number of lines, and save that as the end point\n",
    "                        for start, read, end in zip(start_l_col, lines_col, end_l_col):\n",
    "                            conf_df.loc[index, end] = conf_df.loc[index, start] + conf_df.loc[index, read]\n",
    "                        # C-5-e-2 - then save today's ending point as the starting point for the next day, to be accessed later\n",
    "                        for end, next_start in zip(end_l_col, next_start_l_col):\n",
    "                            conf_df.loc[index, next_start] = conf_df.loc[index, end]\n",
    "                        # C-5-e-3 - save the end line just calculated as object 'end_line'\n",
    "                        end_line = conf_df.loc[index, f'r{rt}d{day}_l_end']\n",
    "\n",
    "                        # debugging line - remove later\n",
    "                        print(f'Readthrough {rt} day {day} end line set. End line: {end_line}')\n",
    "\n",
    "                        # C-5-e-4 - find in the talk the paragraph with a running total that is greater than or equal to the end line \n",
    "                        end_paragraph_1 = talk.loc[talk['running_lines'] >= end_line].index[0]\n",
    "                        # C-5-e-5 - find in the talk the paragraph before the one above\n",
    "                        end_paragraph_2 = talk.loc[talk['running_lines'] >= end_line].index[-1]\n",
    "                        # C-5-e-6 - determine which paragraph would yield a number of lines being read closer to the target end line and save as end_paragraph\n",
    "                        if abs(talk.loc[end_paragraph_1,'running_lines'] - end_line) < abs(talk.loc[end_paragraph_2,'running_lines']- end_line):\n",
    "                            end_paragraph = end_paragraph_1\n",
    "                        else:\n",
    "                            end_paragraph = end_paragraph_2\n",
    "                        # C-5-e-7 - save the selected end_paragraph as the paragraph ending point of the current day of the current readthrough\n",
    "                        for end_p in end_p_col:\n",
    "                            conf_df.loc[index, end_p] = end_paragraph\n",
    "\n",
    "                        for end_p, next_start_p in zip(end_p_col, next_start_p_col):\n",
    "                            conf_df.loc[index, next_start_p] = conf_df.loc[index, end_p] + 1\n",
    "\n",
    "                        # debugging line - remove later\n",
    "                        print(f'Readthrough {rt} day {day} end paragraph set: End paragraph: {end_paragraph}')\n",
    "\n",
    "                # debugging print line - remove later\n",
    "                print(f'Readthrough {rt} columns day {day} saved. Looping back.')\n",
    "            \n",
    "            # debugging print line - remove later\n",
    "            print(f'Readthrough {rt} loop finished.')\n",
    "        \n",
    "        return conf_df\n",
    "    \n",
    "    # runs the get paragraphs function, saves as conf_df_7\n",
    "    conf_df_7 = get_paragraphs(conf_df_6, all_talks_dict)\n",
    "\n",
    "    \n",
    "    ##### Setting up and exporting final breakdown #####\n",
    "    \n",
    "    # establishes patterns for getting a list of start and end columns\n",
    "    start_cols_pat = r'r\\d+d\\d+_p_start'\n",
    "    end_cols_pat = r'r\\d+d\\d+_p_end'\n",
    "\n",
    "    # create a list of columns names in conf_df_7 \n",
    "    cols = list(conf_df_7.columns)\n",
    "\n",
    "    # initialize empty lists for start and end columns\n",
    "    final_start_cols = []\n",
    "    final_end_cols = []\n",
    "\n",
    "    # look at all of the column names in the column names list\n",
    "    for col in cols:\n",
    "        # if the column name matches the pattern for start columns, add it to the list of start columns\n",
    "        if re.search(start_cols_pat, col):\n",
    "            final_start_cols.append(col)\n",
    "        # if the column name matches the pattern for end columns, add it to the list of end columns\n",
    "        elif re.search(end_cols_pat, col):\n",
    "            final_end_cols.append(col)\n",
    "\n",
    "    # initialize an empty list for zipping the other two together\n",
    "        ## zipping two lists together basically entails combining them in the order of list 1 item 1, list 2 item 1, list 1 item 2, list 2 item 2, list 1 item 3, list 2 item 3, etc. \n",
    "    zipped_cols = []\n",
    "\n",
    "    # create a loop that 'zips' the start columns list and end columns list together\n",
    "    for start_col, end_col in zip(final_start_cols, final_end_cols):\n",
    "        zipped_cols.append(start_col)\n",
    "        zipped_cols.append(end_col)\n",
    "    \n",
    "    # create a list of necessary information columns\n",
    "    info_cols = ['Author', 'role', 'Title']\n",
    "\n",
    "    # combine the info_cols list and the zipped_cols list\n",
    "    final_cols = info_cols + zipped_cols\n",
    "\n",
    "    # keep only the columns in the final_cols list, save as final_breakdown\n",
    "    final_breakdown = conf_df_7[final_cols]\n",
    "\n",
    "    # Get user input about saving breakdown to computer\n",
    "    response_1 = input('Do you want to save the breakdown to your computer? ')\n",
    "    \n",
    "    # if the user wants to export the breakdown\n",
    "    if response_1.lower() == 'yes':\n",
    "        # ask the user where they want to store it\n",
    "        input_path = input(f'Please paste the location of the folder you would like to save the breakdown in: \\n'\n",
    "                           f'\\nFor example: D:\\Faith and Religion Stuff\\Come, Follow Me')\n",
    "        # add \\\\ to that path to make it compatible\n",
    "        path_start = f'{input_path}\\\\'\n",
    "        # ask the user if they want to use a custom name\n",
    "        response_2 = input('Would you like to save the file with a custom name?')\n",
    "        # if they do, ask the user for the custom name\n",
    "        if response_2.lower() == 'yes':\n",
    "            custom_name = input('Please enter the name you would like to save the file as: ')\n",
    "            name = f'{custom_name}.csv'\n",
    "        # otherwise generate a generic file name\n",
    "        else:\n",
    "            name = f'{month_year}_breakdown.csv'\n",
    "        \n",
    "        # combine the destination folder with the name of the file\n",
    "        final_path = os.path.join(path_start, name)\n",
    "        \n",
    "        # export final_breakdown as a csv to the destination folder\n",
    "        final_breakdown.to_csv(final_path, index=False)\n",
    "        \n",
    "    # Display the final breakdown for viewing in this notebook\n",
    "    return final_breakdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to document my progress here. \n",
    "\n",
    "Importing packages and getting user input for the link works and the start and end dates works. \n",
    "\n",
    "Navigating to the link and pulling initial information about each item listed on the conference landing page works. \n",
    "\n",
    "Getting rid of fluff like the audit report, session videos, and sustaining of the officers of the Church works. \n",
    "\n",
    "The collection of additional information about each talk works. Current run-time is about 2m45s.\n",
    "\n",
    "Adding primary keys and read and day weights works. Current run-time is about 3m19s.\n",
    "\n",
    "Storing information about the paragraph line lengths of each talk works like the original notebook does. Current runtime is about 9m. \n",
    "\n",
    "Distributing days across readthroughs works. Current runtime is still about 9m.\n",
    "\n",
    "Distributing lines across each day of each readthrough works. Current run time is about 9m40s. \n",
    "\n",
    "Distributing paragraphs across start and end columns based on distributed lines works. Current runtime is 9m3s. \n",
    "\n",
    "Subsetting the dataframe to only include Author, role, Title, and the start and end columns works. Current runtime is 9m25s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakdown_gc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 Oct 2024\n",
    "\n",
    "Functionality issues to address/fix:\n",
    "* the calculator is printing out days after the readthrough is supposed to be finished. Currently, on these days, it prints out the start point as the paragraph after the final paragraph of the talk, and the end point as the final paragraph of the talk. \n",
    "* when exporting to a csv, special characters (like ones with accent marks) seem to be saving in something akin to unicode.\n",
    "* consider also saving the total number of paragraphs in the final breakdown, and maybe use short role instead of role.\n",
    "* make sure code can handle handle different and incorrect inputs (such as different date formats or just putting the wrong thing in as input)\n",
    "\n",
    "Efficiency issues to fix:\n",
    "* adjust function to save conference talks as their own information and draw information about them from it's memory, not export then re-import them. \n",
    "* adjust function to only drive to the web one time, rather than driving to the web to get information about the conference as a whole, then to navigate to it a second time to get information about each of the talks (so include the work done in the `get_talks` functions in the initial driver navigation)\n",
    "* remove unnecessary debugging print lines, implement more informative and useful ones until they are no longer needed. \n",
    "\n",
    "Later features:\n",
    "* enable the function to create a breakdown regardless of any information being missing, like start or end date, number of readthroughs, etc. \n",
    "* enable the function to be able to account for \"break days\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
